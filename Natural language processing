/*           Main approaches              */
1. Rule-based method (manually, time consuming but accurate)
Regular expression
Context-free grammars
2. Probabilistic modeling and machine learning
Semantic slot filling: CRF-conditional random field
3. Deep learning
Semantic slot filling: CRF-conditional random field: LSTM
/*          Linguistic Knowledge              */
morphology形态学: study of words, part of word such as stems, rood words, preffixes, surffixes
syntax句法: relationship between words
semantics语义: synthetics structures, about the meaning
pragmatics语用学: highest level
Tools: 
NLTK (Natural Language Toolkit in python), stanford parser (sythentic analysis),library spaCy, Genism and MALLET for higher level
/*          Text classification             */
Preprocessing: 
1. tokenization (useful unit for semantic processing)
nltk.tokenize.WhitespaceTokenizer()
nltk.tokenize.TreebankWordTokenizer()
nltk.tokenize.WordPunctTokenizer()
2. normalization
2.1 stemming (get root form of word)
Porter's stemmer, nltk.stem.PorterStemm
2.2 lemmatization (get original or dictionary form of word)
WordNet lemmatizer, nltk.stem.WordNetLemmatizer
2.3 Further normalize
normalize captical letters (in the beginning of a sentence, in title)
Acronyms

/*                  Feature exaction                 */
Token to features 
1. Bag of words, each token has a text vectorization (lose order), which is the counts of each token
2. n-grams for token (token pairs or triples, etc.) and remove n-grams 
remove high frequency n-grams (a, the, and, etc.)
remove low frequency n-grams, otherwise overfit 
keep median frequency n-grams
3. rank n-grams
smaller frequency n-grams more discriminating
Term frequency (TF) frequency of term t in document d
Inverse document frequency (IDF) documents contain term t in all documents set
TF-IDF=TF*IDF
4. better BOW by replace count by TF-IDF/l2 norm
week1/tfidf_demo.ipynb 


/*          Linear model for sentiment analysis               */
Logistic regression: handle sparsity, fast to train, weights can be interpreted
Make it better: try tokenization, normalization, try different model, or throw away Bag of Word and use NN

/*          Hash trick in spam filtering                    */
hash(token) %2^b
