/***          Intro and Main approaches              ***/
1. Rule-based method (manually, time consuming but accurate)
Regular expression
Context-free grammars
2. Probabilistic modeling and machine learning
Semantic slot filling: CRF-conditional random field
3. Deep learning
Semantic slot filling: CRF-conditional random field: LSTM
/*   Linguistic Knowledge  */
morphology形态学: study of words, part of word such as stems, rood words, preffixes, surffixes
syntax句法: relationship between words
semantics语义: synthetics structures, about the meaning
pragmatics语用学: highest level
Tools: 
NLTK (Natural Language Toolkit in python), stanford parser (sythentic analysis),library spaCy, Genism and MALLET for higher level
/***          Text classification             ***/
Preprocessing: 
1. tokenization (useful unit for semantic processing)
nltk.tokenize.WhitespaceTokenizer()
nltk.tokenize.TreebankWordTokenizer()
nltk.tokenize.WordPunctTokenizer()
2. normalization
2.1 stemming (get root form of word)
Porter's stemmer, nltk.stem.PorterStemm
2.2 lemmatization (get original or dictionary form of word)
WordNet lemmatizer, nltk.stem.WordNetLemmatizer
2.3 Further normalize
normalize captical letters (in the beginning of a sentence, in title)
Acronyms

/*   Feature exaction   */
Token to features, each document has a list of features
1. Bag of words, each token has a text vectorization (lose order), which is the counts of each token
2. n-grams for token (token pairs or triples, etc.) and remove n-grams 
remove high frequency n-grams (a, the, and, etc.)
remove low frequency n-grams, otherwise overfit 
keep median frequency n-grams
3. rank n-grams
smaller frequency n-grams more discriminating
Term frequency (TF) frequency of term t in document d
Inverse document frequency (IDF) documents contain term t in all documents set
TF-IDF=TF*IDF
4. better BOW by replace count by TF-IDF/l2 norm
week1/tfidf_demo.ipynb 


/*    Linear model for sentiment analysis    */
Logistic regression: handle sparsity, fast to train, weights can be interpreted
Make it better: try tokenization, normalization, try different model, or throw away Bag of Word and use NN

/*    Hash trick in spam filtering    */
small dataset: store n-gram feature index in hash table
large dataset: out of memory, hard to synchronize
n-grams --> f0(token)=hash(n gram token) %2^b 
can add personize function: fu(user+'_'+token) % 2^b


/***           Neural Networks for words                ***/
Word2vec word embedding transfer a word to a vector of length n
A convolutional filter (dim 2*n) of size 2 dot multiply the word embedding pairs (dim 2*n) gives out a result
It is better than BOW of n grams, because a good convolutional filter can get higher meaning of different words to similar value.
1D convolution (slide window only in one direction) outputs same length with input, so choose the maximum as activation, maximum 
pulling over time
Application: 3,4,5 gram-window with 100 filter each, then get 300 features in one vector, then do further multilayer perceptron (MLP)
GOAL: get fixed number of features with dynamic length of text and capture the meanings in the text

/*   Neural Networks for characters   */
1D convolution on character: transfer character to sparse vector of fixed length, then try different filters to get vector with 
same length (num of characters) with text, then max pooling (choose maximum every two character)
Repeat 1D convolution and pooling 
